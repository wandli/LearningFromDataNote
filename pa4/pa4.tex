\documentclass{article}
\usepackage{amsmath}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx,algorithm}
\begin{document}
(cartpole) Consider an inverted pendulum tied to a cart, our goal is to stabilize it by moving the cart
to the left or to the right.

Now we formulate the problem in classical mechanics and Reinforcement Learning terminology.
We choose the length of the rod $L = 0.5 m$, the mass of the cart $M=1 kg$, the mass of the ball $m=0.1 kg$
and there is no friction between the cart and the floor.
The state of the system is described by
$(x, \theta, \dot{x}, \dot{\theta})$. Notice that $(x, \dot{x})$ is the position of velocity of the cart while
$(\theta, \dot{\theta})$ is the relative position and angular velocity of the ball with respect to the cart.

The action we can do is applying a force
$F$ or $-F$. We specify that the right direction is positive. To make the problem concrete,
we require $F=10N$ and we discrete the time such that at time $t+\Delta t$ we can choose another action.
$\Delta t = 0.02s$. The new state $(x', \theta', \dot{x}', \dot{\theta}')$ is updated from old state by the following law:
\begin{align*}
x' &= x + \Delta t \cdot \dot{x} \\
\theta' &= \theta + \Delta t \cdot \dot{\theta} \\
\dot{x}' &= \dot{x} + \Delta t \cdot \ddot{x} \\
\dot{\theta}' &= \dot{\theta} + \Delta t \cdot \ddot{\theta}
\end{align*}
The acceleration $(\ddot{x}, \ddot{\theta})$ can be computed from Newton's law.
We list its equation here for convenience:
\begin{align*}
& (m + M)\ddot{x} + mL(\ddot{\theta} \cos \theta - \dot{\theta}^2 \sin \theta) = F \\
& m l \ddot{\theta} + m \ddot{x} \cos\theta = m g \sin \theta \\
\end{align*}

There is a reward $+1$ each time the ball is in the upright position, otherwise the reward is zero.
In computation, we should set a threshold for the upright position, when $|\theta|< \delta \theta = 12^\circ$.

We also set the termination condition when $|\theta| > \delta' \theta = 45^\circ$ and $|x| > \delta x = 4.8 m $.

Now we are ought to help it learn from the reward matrix, thus it can eat the cheese on the $(3,3)$ and avoid traps on $(1,1)$, $(1,3)$ and $(3,1)$. Recall the \emph{Q-learning} you learned from class, the mouse itself has to build a Q-table $Q(s,a)$ where $s$ is the state and $a$ is the action, then it can decide which action to take by this Q-table. Updating this Q-table is commonly performed as algorithm \ref{alg1}:

\begin{algorithm}[b] 
	\caption{Deep Q learning \label{alg1}} %算法的名字
	\hspace*{0.02in} {\bf Input:} %算法的输入， \hspace*{0.02in}用来控制位置，同时利用 \\ 进行换行
	Initialize $Q(s,a,w)$ arbitrarily
	\begin{algorithmic}[1]
		\For{for each episode}
		\State Initialize $s$
		\While{$s$ is not terminal}
		\State Choose $a$ from $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
		\State Take action $a$, observe $r$, $s^{\prime}$
		\State $\hat{Q}(s,a) \gets Q(s,a, w) + \alpha \left ( r + \gamma \max_{a^{\prime}}Q(s^{\prime},a^{\prime}, w) - Q(s, a, w) \right )$
		\State $w' = \arg\min_{w}|| Q(s,a, w) - \hat{Q}(s,a)||^2$
		\State $s\gets s^{\prime}, w \gets w'$
		\EndWhile
		\EndFor
	\end{algorithmic}
\end{algorithm}

%\begin{thebibliography}{9}
%	\bibitem{cartpole} Florian, Razvan V. "Correct equations for the dynamics of the cart-pole system." Center for Cognitive and Neural Studies (Coneural), Romania (2007).
%\end{thebibliography}
\end{document}